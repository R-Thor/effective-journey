1)      BUILD:
            podman build -f Containerfile -t llama.cpp-runtime:alpha .
2)      START:
            DEBUG:
                podman run    --rm -it --name llama.cpp --network host -v ~/workspace/ai/models/gguf:/models:Z llama.cpp-runtime:alpha /opt/llama.cpp/build/bin/llama-server --model /models/llama-2-7b.Q4_K_M.gguf --host 0.0.0.0 --port 8080
            DEAMON:
                podman run -d --rm     --name llama.cpp --network host -v ~/workspace/ai/models/gguf:/models:Z llama.cpp-runtime:alpha /opt/llama.cpp/build/bin/llama-server --model /models/llama-2-7b.Q4_K_M.gguf --host 0.0.0.0 --port 8080
            LOGIN:
                podman exec -it llama.cpp bash
            CURL:
                date; curl -X POST http://localhost:8080/completion -H "Content-Type: application/json" -d '{"prompt":"Hello","n_predict":32}';echo; date
3)      TESTS

3.1)    INTERNAL

3.1.1)  podman exec -it gracious_torvalds bash
        /opt/llama.cpp/build/bin/llama-cli --help
        RESULT: PASS

3.1.2)  /opt/llama.cpp/build/bin/llama-cli -m /models/llama-2-7b.Q4_K_M.gguf -p "Hello"
        RESULT: PASS

3.1.3)  ./llama-completion -m /models/llama-2-7b.Q4_K_M.gguf -p "Hello"
        /opt/llama.cpp/build/bin/llama-completion -m /models/llama-2-7b.Q4_K_M.gguf -p "Hello"
        RESULT: PASS

3.2)    EXTERNAL
3.2.1)  step 1> on host > podman run --rm -it --security-opt label=disable -p 8080:8080 -v ~/workspace/ai/models/gguf:/models:Z llama.cpp-runtime:alpha /opt/llama.cpp/build/bin/llama-server --model /models/llama-2-7b.Q4_K_M.gguf --host 0.0.0.0 --port 8080
        step 2> on host > date; curl -X POST http://localhost:8080/completion -H "Content-Type: application/json" -d '{"prompt":"Hello","n_predict":32}';echo; date

        RESULT: PASS
